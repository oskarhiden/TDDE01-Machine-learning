return(res)
}
log_p(1990, 10)
log_p(1990, 710)
log_p(1990, 1910)
log_p()
log(710)
log(1990)
log_p = function(y, lambda){
res = -lambda + y*log(lambda)- sum(log(factorial(1:y)))
return(res)
}
log_p(1990, 1910)
-710 + 1990*log(710) - sum(log(factorial(1:710)))
log_p = function(y, lambda){
res = -lambda + y*log(lambda)- sum(log(1:y))
return(res)
}
log_p(1990, 1910)
loglike = 0
for(i in 1:length(influenza$Mortality)){
loglike = loglike + log_p(influenza$Mortality[i], 2910)
}
loglike
log_p = function(y, lambda){
res = -lambda + y*log(lambda)- sum(log(1:y))
return(-res)
}
loglike = 0
for(i in 1:length(influenza$Mortality)){
loglike = loglike + log_p(influenza$Mortality[i], 2910)
}
loglike
loglike = numeric(length(lambda))
loglike = numeric(length(lambda))
for(j in 1:length(lambda)){
for(i in 1:length(influenza$Mortality)){
loglike[j] = loglike + log_p(influenza$Mortality[i], labda[j])
}
}
loglike = numeric(length(lambda))
for(j in 1:length(lambda)){
for(i in 1:length(influenza$Mortality)){
loglike[j] = loglike + log_p(influenza$Mortality[i], lambda[j])
}
}
loglike
plot(loglike)
plot(x = lambda, y = loglike)
which.max(loglike)
lambda[which.max(loglike)]
hist(rpois((1:20), 110))
# Step 1
hist(sort(influenza$Mortality, decreasing=T), breaks = 50)
hist(rpois((1500:2600), 110))
loglike
plot(x = lambda, y = loglike)
lambda[which.max(loglike)]
lambda[which.min(loglike)]
#hist(rpois((1500:2600), 110))
log=0
for(i in 1:length(influenza$Mortality)){
log = loglike + log_p(influenza$Mortality[i], lambda[1])
}
log
#hist(rpois((1500:2600), 110))
log=0
for(i in 1:length(influenza$Mortality)){
log = log + log_p(influenza$Mortality[i], lambda[1])
}
log
loglike
final_lasso = glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson", lambda = lambda_min)
predict(final_lasso, newdata = test)
predict(final_lasso, newx = test[-3])
predict(final_lasso, newx = as.matrix(test[-3]))
pred_test = predict(final_lasso, newx = as.matrix(test[-3]))
pred_test = predict(final_lasso, newx = as.matrix(test[,-3]))
MSE = ((pred_test - test[,3])^2)/length(pred_test)
MSE
MSE
MSE = sum((pred_test - test[,3])^2)/length(pred_test)
MSE
lasso_mse = cv.glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson")
plot(lasso_mse)
lambda_min = lasso_mse$lambda.min
log(lambda_min)
lasso_mse$glmnet.fit
final_lasso$a0
View(final_lasso)
View(final_lasso)
final_lasso
final_lasso$df
final_lasso$offset
final_lasso$df
final_lasso$dfmat
final_lasso$beta
final_lasso@a0
final_lasso$a0
final_lasso$a0 # intercept
exp(final_lasso$a0)
loglike
plot(x = lambda, y = loglike)
lambda[which.min(loglike)]
lambda[which.min(loglike[-1])]
lambda[which.min(loglike[-1])+1]
best_lambda = lambda[which.min(loglike[-1])+1]
best_lambda
exp(final_lasso$a0)
# Step 1
hist(sort(influenza$Mortality, decreasing=T), breaks = 50)
y = (lambda^x) * e^(-lambda) / factorial(x)
# they are similar,
x = (1:2600)
lambda = best_lambda
y = (lambda^x) * e^(-lambda) / factorial(x)
lambda = best_lambda
y = (lambda^x) * exp(-lambda) / factorial(x)
plot(y)
# Step 1
hist(sort(influenza$Mortality, decreasing=T), breaks = 50)
exp(final_lasso$a0)
best_lambda
final_lasso$a0 # intercept
mean(influenza$Mortality)
best_lambda
fianl_lasso$beta #lasso coefi
final_lasso$df #number of vaiabls used
fianl_lasso$beta #lasso coefi
final_lasso$beta #lasso coefi
# they are similar, the exp(intercept) is simmilar to the best lambda(close to sample mean),
log(1600)
log(2600)
exp(final_lasso$a0)
best_lambda
mean(influenza$Mortality)
exp(final_lasso$a0)
best_lambda
mean(influenza$Mortality)
# Step 3 - regression tree
library(tree)
?tree
tree_model = tree(Mortality ~ ., data = train)
tree_model
text(tree_model)
plot(tree_model)
text(tree_model)
cv.tree(tree_model)
cv_tree = cv.tree(tree_model)
cv.tree
plot(cv.tree)
plot(cv_tree)
final_lasso$beta #lasso coefi
log(0.022)
exp(0.022)
plot(cv_tree)
cv_tree = cv.tree(tree_model)
plot(cv_tree)
plot(tree_model)
text(tree_model)
cv_tree = cv.tree(tree_model)
plot(cv_tree)
cv_tree$dev
cv_tree
# best tree = 8 levs
best_tree = prune.tree(tree_model, best =8)
pred_tree = predict(best_tree, newdata = test)
pred_tree
MSE_tree = sum((pred_tree - test$Mortality)^2)/length(pred_tree)
MSE_tree
# Compare MSE
MSE
MSE_tree
# Compare MSE
MSE
MSE_tree
MSE = sum((pred_test - test$Mortality)^2)/length(pred_test)
MSE
plot(test$Mortality)
points(pred_tree, col="green")
points(pred_test, col="blue")
points(pred_test, col="blue")
plot(test$Mortality, ylim=c(1000, 3000))
points(pred_tree, col="green")
points(pred_test, col="blue")
plot(pred_test, col="blue")
plot(exp(pred_test), col="blue")
points(exp(pred_test), col="blue")
plot(test$Mortality, ylim=c(1000, 3000))
points(pred_tree, col="green")
points(exp(pred_test), col="blue")
plot(test$Mortality))
points(pred_tree, col="green")
points(exp(pred_test), col="blue")
plot(test$Mortality)
points(pred_tree, col="green")
points(exp(pred_test), col="blue")
lasso = glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson")
plot(lasso, xvar = "lambda", label = T)
lasso_mse = cv.glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson")
plot(lasso_mse)
lambda_min = lasso_mse$lambda.min
final_lasso = glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson", lambda = lambda_min)
pred_test = predict(final_lasso, newx = as.matrix(test[,-3]))
MSE = sum((pred_test - test$Mortality)^2)/length(pred_test)
MSE
lambda_min
MSE = sum((exp(pred_test) - test$Mortality)^2)/length(pred_test)
MSE
# Compare MSE
MSE
MSE_tree
plot(test$Mortality)
points(pred_tree, col="green")
points(exp(pred_test), col="blue")
pred_test
train[,3]
lambda_min
exp(lambda_min)
?cv.glmnet
lasso_mse$cvm
exp(lasso_mse$cvm)
# Step 4
pca_data = influenza
pca_data$Mortality = c()
prcomp(pca_data)
pca = prcomp(pca_data)
lambda = pca$sdev^2
sprintf("%2.3f",lambda/sum(lambda)*100)
sprintf("%2.3f",cumsum(lambda)/sum(lambda)*100)
pca$
pca$rotation
pca$rotation
pca$x
pca_data = pca$x
pca_data$PC1
str(pca_data)
pca_data[,1]
PC_2 = pca_data[,2]
PC_1 = pca_data[,1]
PC_2 = pca_data[,2]
x = cbind(PC_1, PC_2)
x
str(x)
x = as.matrix(cbind(PC_1, PC_2))
str(x)
# Step 4
pca_data = train
pca_data$Mortality = c()
pca = prcomp(pca_data)
lambda = pca$sdev^2
sprintf("%2.3f",cumsum(lambda)/sum(lambda)*100)
x = pca_data[, (1:5)]
x4
x
pca_data
pca$x
# Step 4
pca_data = train
pca_data$Mortality = c()
pca = prcomp(pca_data)
lambda = pca$sdev^2
sprintf("%2.3f",cumsum(lambda)/sum(lambda)*100)
pca$rotation
pca_data = pca$x
pca_data$PC1
x = pca_data[, (1:5)]
str(x)
#lasso
lasso_model_pca = glmnet(x, )
#lasso
lasso_model_pca = glmnet(x, train$Mortality, alpha=1, family= "poisson" )
lasso_model_pca
lasso_model_pca = cv.glmnet(x, train$Mortality, alpha=1, family= "poisson" )
lasso_model_pca$lambda.min
lambda_min_pca = lasso_model_pca$lambda.min
lasso_model_pca = cv.glmnet(x, train$Mortality, alpha=1, family= "poisson", lambda = seq(0, 50, by=0.1) )
lambda_min_pca = lasso_model_pca$lambda.min
lambda_min_pca
plot(lasso_model_pca)
# No, higher values of
log(50)
log(1)
# No, higher values of lambda makes the model more simple by removing number of pca components
# used in the lasso-regression
coef(lasso_model_pca, s=lambda_min_pca)
coef(glmnet(x, train$Mortality, alpha=1, family= "poisson" , lambda=lambda_min_pca))
#Assignment 2 - Neural Network
install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 3)
tr <- data.frame(Var, Sin=sin(Var))
Var <- runif(50, 3, 9)
install.packages("neuralnet")
te <- data.frame(Var, Sin=sin(Var))
install.packages("neuralnet")
#Assignment 2 - Neural Network
install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 3)
tr <- data.frame(Var, Sin=sin(Var))
Var <- runif(50, 3, 9)
te <- data.frame(Var, Sin=sin(Var))
set.seed(1234567890)
Var <- runif(50, 0, 3)
tr <- data.frame(Var, Sin=sin(Var))
Var <- runif(50, 3, 9)
te <- data.frame(Var, Sin=sin(Var))
View(te)
View(te)
winit = runif(20, -1, 1)
?neuralnet
View(te)
View(te)
nn = nerualnet(Sin ~ Var, data = tr, hidden = c(3), sartweight = winit)
library(neuralnet)
nn = nerualnet(Sin ~ Var, data = tr, hidden = c(3), sartweight = winit)
?neuralnet
nn = neuralnet(Sin ~ Var, data = tr, hidden = c(3), sartweight = winit)
nn = neuralnet(Sin ~ Var, data = tr, hidden = c(3), startweights = winit)
pred = predict(nn, newdata = tr)
pred = predict(nn, newdata = te)
plot(te)
points(x=te$Var, y=pred, col="red")
plot(te, ylim=c(-2,2))
points(x=te$Var, y=pred, col="red")
points(te)
points(te, col=blue)
plot(te, ylim=c(-2,2))
points(x=te$Var, y=pred, col="red")
points(te, col="blue")
plot(tr, ylim=c(-2,2))
points(x=te$Var, y=pred, col="red")
points(te, col="blue")
plot(tr, ylim=c(-2,2), xlim=c(0, 8))
points(x=te$Var, y=pred, col="red")
points(te, col="blue")
# Step 2
# nn weights:
nn$weights
nn
# Step 2
# nn weights:
nn$weights
?sigmoid
?sigmoid
?nerualnet
?neuralnet
plot(nn)
nn
View(nn)
View(nn)
nn$err.fct()
nn$err.fct
plot(nn)
# calculate hidden units for data:
logistic_sigmoid = function (x){
return(1/(1+exp(-x)))
}
nn$weights
z1 = logistic_sigmoid(te$Var*0.6170477-1.5198771)
z1
z1 = logistic_sigmoid(te$Var*0.6170477-1.5198771)
z2 = logistic_sigmoid(1.995*te$Var-1.27708)
z3 = logistic_sigmoid(-1.61733*te$Var+4.89639)
y=-3.92871*z1+2.67522*z2+0.84607*z3-0.62953
print(y)
x = te$Var
z1 = logistic_sigmoid(x*0.6170477-1.5198771)
z2 = logistic_sigmoid(1.995*x-1.27708)
z3 = logistic_sigmoid(-1.61733*x+4.89639)
y=-3.92871*z1+2.67522*z2+0.84607*z3-0.62953
print(y)
x = sort(x)
x
z1 = logistic_sigmoid(x*0.6170477-1.5198771)
z2 = logistic_sigmoid(1.995*x-1.27708)
z3 = logistic_sigmoid(-1.61733*x+4.89639)
y=-3.92871*z1+2.67522*z2+0.84607*z3-0.62953
print(y)
z1
z2
plot(z1, ylim = c(0,1))
points(z2, col="blue")
points(z3, col"red")
plot(z1, ylim = c(0,1))
points(z2, col="blue")
points(z3, col="red")
print(y)
plot(z1, ylim = c(0,1))
plot(z1, ylim = c(0,1))
points(z2, col="blue")
points(z3, col="red")
print(y)
-3.92871+2.67522-0.62953
?ksvm
library(kernlab)
?ksvm
?glmnet
best_lambda
RNGversion('3.5.1')
library(readr)
influenza = read.csv("C:/Users/oskar/OneDrive/Universitet/Luleå Tekniska högskola/Databaser 1/Dokument/Git Repro/TDDE01-Machine-learning/Tentor/exam_without_solutions - TDDE01/Influenza.csv")
#video = read.csv("Desktop/video.csv")
# Step 1
hist(sort(influenza$Mortality, decreasing=T), breaks = 50)
lambda = seq(10, 2910, by=100)
log_p = function(y, lambda){
res = -lambda + y*log(lambda)- sum(log(1:y))
return(-res)
}
loglike = numeric(length(lambda))
for(j in 1:length(lambda)){
for(i in 1:length(influenza$Mortality)){
loglike[j] = loglike + log_p(influenza$Mortality[i], lambda[j])
}
}
loglike
plot(x = lambda, y = loglike)
best_lambda = lambda[which.min(loglike[-1])+1]
# Step 2
scaled_data = influenza
scaled_data [ , -3] = scale(scaled_data[, -3])
data = scaled_data
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
library(glmnet)
lasso = glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson")
plot(lasso, xvar = "lambda", label = T)
lasso_mse = cv.glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson")
plot(lasso_mse)
lambda_min = lasso_mse$lambda.min
final_lasso = glmnet(as.matrix(train[,-3]), train[,3], alpha=1, family= "poisson", lambda = lambda_min)
pred_test = predict(final_lasso, newx = as.matrix(test[,-3]))
MSE = sum((exp(pred_test) - test$Mortality)^2)/length(pred_test)
MSE
# Is MSE actually the best way to measure error rate?
# No since Max-likelihood is the same as min MSE only when pred is normaly distributed.
# instead poisson deviance should be used.
final_lasso$df #number of vaiabls used
final_lasso$beta #lasso coefi
# biggest impact would be Influenza_lag2, influenza and temp deficit
final_lasso$a0 # intercept
exp(final_lasso$a0)
best_lambda
mean(influenza$Mortality)
# they are similar, the exp(intercept) is simmilar to the best lambda(close to sample mean),
log(1600)
log(2600)
best_lambda
# Step 3 - regression tree
library(tree)
tree_model = tree(Mortality ~ ., data = train)
plot(tree_model)
text(tree_model)
cv_tree = cv.tree(tree_model)
plot(cv_tree)
cv_tree$dev
cv_tree
# best tree = 8 levs
best_tree = prune.tree(tree_model, best =8)
pred_tree = predict(best_tree, newdata = test)
MSE_tree = sum((pred_tree - test$Mortality)^2)/length(pred_tree)
# Compare MSE
MSE
MSE_tree
plot(test$Mortality)
points(pred_tree, col="green")
points(exp(pred_test), col="blue")
# In a tree we do varible selection by limiting the number of leavs in the tree. The varibles
# that explains most of the variation is used.
# Step 4
pca_data = train
pca_data$Mortality = c()
pca = prcomp(pca_data)
lambda = pca$sdev^2
sprintf("%2.3f",cumsum(lambda)/sum(lambda)*100)
# 5 principal components is needed
pca$rotation
pca_data = pca$x
pca_data$PC1
x = pca_data[, (1:5)]
str(x)
#lasso
lasso_model_pca = glmnet(x, train$Mortality, alpha=1, family= "poisson" )
lasso_model_pca
lasso_model_pca = cv.glmnet(x, train$Mortality, alpha=1, family= "poisson", lambda = seq(0, 50, by=0.1) )
lambda_min_pca = lasso_model_pca$lambda.min
lambda_min_pca #5.2
plot(lasso_model_pca) # error vs log(lambda)
# No, higher values of lambda makes the model more simple by removing number of pca components
# used in the lasso-regression
coef(lasso_model_pca, s=lambda_min_pca)
coef(glmnet(x, train$Mortality, alpha=1, family= "poisson" , lambda=lambda_min_pca))
# 3 PC-components are used!
best_lambda
hist(rpois(50, best_lambda))
?hist
hist(rpois(50, best_lambda), breaks = 4)
hist(rpois(50, best_lambda), breaks = 4)
hist(rpois(250, best_lambda), breaks = 20)
hist(rpois(2500, best_lambda), breaks = 20)
hist(dpois(2500, best_lambda), breaks = 20)
hist(dpois(2500, best_lambda), breaks = 20)
hist(ppois(2500, best_lambda), breaks = 20)
?ksvm
kernlab
