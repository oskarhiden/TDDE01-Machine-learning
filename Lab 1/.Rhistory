mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
#MISSING: check formulas for linear regression and compute beta
#(Xt*X)^-1 * Xt * y
X = cbind(1, X)
beta = solve(t(X)%*%X, t(X)%*%Y)
Res=Xpred1%*%beta
return(Res)
}
mylin(X1, Y1, X1)
Y1
X = as.matrix(swiss[,2:6])
Y = swiss[[1]]
Nfolds = 5
# myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):length(X1)
}
X_train = X1[!indices,]
Y_train = Y1[!indices]
X_validate = X1[indices]
Yp = X1[indices]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#linear regression and returns predicted Y
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
#MISSING: check formulas for linear regression and compute beta
#minimizing least sqeare givew following formula: w_hat = (Xt*X)^-1 * Xt * y
X = cbind(1, X)
beta = solve(t(X)%*%X, t(X)%*%Y)
Res=Xpred1%*%beta
return(Res)
}
X = as.matrix(swiss[,2:6])
Y = swiss[[1]]
Nfolds = 5
# myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):length(X1)
}
X_train = X1[!indices,]
Y_train = Y1[!indices]
X_validate = X1[indices]
Yp = X1[indices]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):length(X1)
}
X_train = X1[!indices,]
Y_train = Y1[!indices]
X_validate = X1[indices,]
Yp = X1[indices]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
X1[indices]
X1[!indices]
X1
X1[!indices,]
X1[indices,]
X1[!indices,]
X1
indices
print(indices)
X_train = X1[!indices,]
Y_train = Y1[!indices]
X_validate = X1[indices,]
Yp = Y1[indices]
print(X_train)
print(Y_train)
head(X_train)
X_train = X1[-indices,]
Y_train = Y1[-indices]
X_validate = X1[indices,]
Yp = Y1[indices]
print(X_train)
print(Y_train)
X = as.matrix(swiss[,2:6])
Y = swiss[[1]]
Nfolds = 5
# myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):length(X1)
}
print(indices)
X_train = X1[-indices,]
Y_train = Y1[-indices]
X_validate = X1[indices,]
Yp = Y1[indices]
#print(X_train)
#print(Y_train)
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):length(X1)
}
print(indices)
X_train = X1[-indices,]
Y_train = Y1[-indices]
X_validate = X1[indices,]
Yp = Y1[indices]
#print(X_train)
#print(Y_train)
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):n
}
print(indices)
X_train = X1[-indices,]
Y_train = Y1[-indices]
X_validate = X1[indices,]
Yp = Y1[indices]
#print(X_train)
#print(Y_train)
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
i=which.min(MSE)
Features
model
feature
Features
MSE
Nfeat
X1
X1[X1[,5]>20]
X1[X1[,5]>20,]
model
which(model=1)
which(model==1)
X = as.matrix(swiss[,2:6])
Y = swiss[[1]]
Nfolds = 5
# myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):n
}
print(indices)
X_train = X1[-indices, which(model == 1)]
Y_train = Y1[-indices]
X_validate = X1[indices, which(model == 1)]
Yp = Y1[indices]
#print(X_train)
#print(Y_train)
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
MSE
plot(MSE)
plot(Nfeat,MSE)
#MISSING: plot MSE against number of features
plot(Nfeat, MSE, main = "MSE vs number of feat.")
#MISSING: plot MSE against number of features
plot(Nfeat, MSE, main = "MSE vs number of feat.", xlab = "Number of features")
#linear regression and returns predicted Y
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
#MISSING: check formulas for linear regression and compute beta
#minimizing least sqeare givew following formula: w_hat = (Xt*X)^-1 * Xt * y
X = cbind(1, X)
beta = solve(t(X)%*%X, t(X)%*%Y)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):n
}
print(indices)
X_train = X1[-indices, which(model == 1)]
Y_train = Y1[-indices]
X_validate = X1[indices, which(model == 1)]
Yp = Y1[indices]
#print(X_train)
#print(Y_train)
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
print(model)
print(k)
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(Nfeat, MSE, main = "MSE", xlab = "Number of features")
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):n
}
print(indices)
X_train = X1[-indices, which(model == 1)]
Y_train = Y1[-indices]
X_validate = X1[indices, which(model == 1)]
Yp = Y1[indices]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(Nfeat, MSE, main = "MSE", xlab = "Number of features")
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
if(k!=Nfolds){
indices = ((k-1)*sF):(k*sF)
}else{
indices = ((k-1)*sF):n
}
X_train = X1[-indices, which(model == 1)]
Y_train = Y1[-indices]
X_validate = X1[indices, which(model == 1)]
Yp = Y1[indices]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
Ypred = mylin(X_train, Y_train, X_validate)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(Nfeat, MSE, main = "MSE", xlab = "Number of features")
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
